{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ffb666e",
   "metadata": {},
   "source": [
    "# Understanding MV-VTON: Multi-View Virtual Try-On with Diffusion Models\n",
    "\n",
    "## Key Points of MV-VTON\n",
    "\n",
    "### 1. Core Innovation\n",
    "- **Diffusion Models for Try-On**: First paper to apply diffusion models to virtual try-on, replacing traditional GAN-based approaches\n",
    "- **Multi-View Capability**: Generates consistent try-on results from multiple viewing angles (front, side, back)\n",
    "- **3D-Aware Processing**: Implicitly models 3D clothing deformation without explicit 3D reconstruction\n",
    "\n",
    "### 2. Architecture Overview\n",
    "- **Two-Stage Pipeline**:\n",
    "  1. **Garment Warping Stage**: Aligns the clothing item with the target pose\n",
    "  2. **Try-On Synthesis Stage**: Uses diffusion models to generate photorealistic results\n",
    "\n",
    "- **Multi-View Attention Mechanism**: Ensures consistency across different viewing angles\n",
    "\n",
    "### 3. Key Components\n",
    "- **Diffusion Model Backbone**: Based on Stable Diffusion architecture\n",
    "- **Conditioning Mechanisms**:\n",
    "  - Pose keypoints\n",
    "  - Densepose (body surface representation)\n",
    "  - Textual descriptions of clothing\n",
    "- **Multi-View Consistency Module**: Special attention layers that correlate features across views\n",
    "\n",
    "### 4. Advantages Over Previous Approaches\n",
    "- **Higher Quality Results**: Diffusion models produce more realistic textures and details\n",
    "- **Better Handling of Complex Poses**: More robust to extreme poses than GAN-based methods\n",
    "- **View Consistency**: Maintains clothing appearance across different angles\n",
    "- **Fewer Artifacts**: Reduces common issues like blurring or distortion at seams\n",
    "\n",
    "### 5. Training Approach\n",
    "- **Multi-View Dataset**: Requires images of the same person in multiple poses/angles\n",
    "- **Two-Phase Training**:\n",
    "  1. Pretrain on large-scale fashion dataset\n",
    "  2. Fine-tune on specific try-on datasets\n",
    "- **Loss Functions**:\n",
    "  - Standard diffusion model loss\n",
    "  - Multi-view consistency loss\n",
    "  - Perceptual loss for realism\n",
    "\n",
    "### 6. Performance\n",
    "- Outperforms previous state-of-the-art (VITON-HD, HR-VITON) in both quantitative metrics and user studies\n",
    "- Particularly strong at:\n",
    "  - Preserving clothing patterns/textures\n",
    "  - Handling complex draping effects\n",
    "  - Maintaining body proportions\n",
    "\n",
    "### 7. Limitations\n",
    "- **Computational Requirements**: More demanding than GAN-based approaches\n",
    "- **Inference Speed**: Slower than real-time (though can be optimized)\n",
    "- **Data Requirements**: Needs multi-view data for best results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23dc9fa",
   "metadata": {},
   "source": [
    "## 1. The exact architecture of their diffusion model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eced0f",
   "metadata": {},
   "source": [
    "# MV-VTON Diffusion Model Architecture: Detailed Breakdown\n",
    "\n",
    "## Core Architecture Overview\n",
    "\n",
    "MV-VTON uses a **modified Stable Diffusion** architecture adapted for virtual try-on tasks. The system consists of two interconnected diffusion processes working in tandem:\n",
    "\n",
    "1. **Garment Warping Diffusion** (Conditional U-Net)\n",
    "2. **Try-On Synthesis Diffusion** (Multi-View U-Net)\n",
    "\n",
    "## 1. Garment Warping Stage\n",
    "\n",
    "### Inputs:\n",
    "- Source clothing image (C)\n",
    "- Target pose keypoints (P_t)\n",
    "- Source pose keypoints (P_s)\n",
    "- Densepose representations\n",
    "\n",
    "### Warping Model Components:\n",
    "- **Texture Encoder**: ViT-based encoder extracting multi-scale clothing features\n",
    "  ```python\n",
    "  class TextureEncoder(nn.Module):\n",
    "      def __init__(self):\n",
    "          self.vit = VisionTransformer(patch_size=16, embed_dim=768)\n",
    "          self.multi_scale_proj = nn.ModuleList([\n",
    "              nn.Conv2d(768, 256, 1),\n",
    "              nn.Conv2d(768, 128, 1),\n",
    "              nn.Conv2d(768, 64, 1)\n",
    "          ])\n",
    "  ```\n",
    "- **Flow Prediction Network**: Predicts deformation field\n",
    "  - Uses cross-attention between source and target pose features\n",
    "  - Outputs multi-resolution flow fields (coarse to fine)\n",
    "\n",
    "### Diffusion Process:\n",
    "- Forward process gradually adds noise to the flow field\n",
    "- Reverse process learns to denoise while conditioned on pose\n",
    "- Loss function combines:\n",
    "  ```math\n",
    "  L_{warp} = λ_1L_{flow} + λ_2L_{perc} + λ_3L_{style}\n",
    "  ```\n",
    "\n",
    "## 2. Try-On Synthesis Stage\n",
    "\n",
    "### Base Architecture:\n",
    "Modified U-Net with these key additions:\n",
    "\n",
    "1. **Multi-View Cross-Attention Blocks**:\n",
    "   ```python\n",
    "   class MultiViewAttention(nn.Module):\n",
    "       def __init__(self, channels):\n",
    "           self.query = nn.Linear(channels, channels)\n",
    "           self.key = nn.Linear(channels, channels)\n",
    "           self.value = nn.Linear(channels, channels)\n",
    "           self.multi_view_proj = nn.Linear(channels*3, channels)\n",
    "           \n",
    "       def forward(self, x, view_features):\n",
    "           # x: [B,C,H,W], view_features: [B,N,C]\n",
    "           q = self.query(x.flatten(2))\n",
    "           k = self.key(view_features)\n",
    "           v = self.value(view_features)\n",
    "           # Multi-view attention calculation\n",
    "           ...\n",
    "   ```\n",
    "\n",
    "2. **Pose-Conditioned Residual Blocks**:\n",
    "   - Inject pose information via adaptive normalization\n",
    "   - Uses densepose UV maps as additional conditioning\n",
    "\n",
    "3. **Texture Preservation Modules**:\n",
    "   - Attention gates that reference original garment features\n",
    "   - Multi-scale feature fusion from warping stage\n",
    "\n",
    "### Conditioning Mechanism:\n",
    "- **Concatenated Inputs**:\n",
    "  ```\n",
    "  [Warped_Garment × Body_Segmentation × DensePose × Pose_Keypoints]\n",
    "  ```\n",
    "- **Time-Embedded Diffusion Steps**:\n",
    "  - Sinusoidal positional encoding of timesteps\n",
    "  - Adaptive normalization uses time embedding\n",
    "\n",
    "### Specialized Layers:\n",
    "\n",
    "1. **View-Consistent Denoising**:\n",
    "   - Shared noise prediction across views\n",
    "   - View-specific modulation via attention weights\n",
    "\n",
    "2. **Appearance Transfer Blocks**:\n",
    "   ```python\n",
    "   class AppearanceTransfer(nn.Module):\n",
    "       def __init__(self):\n",
    "           self.adaLN = AdaptiveLayerNorm()\n",
    "           self.texture_attn = CrossAttention()\n",
    "           self.color_proj = nn.Conv2d(3, 64, 1)\n",
    "           \n",
    "       def forward(self, x, garment_features):\n",
    "           color_hint = self.color_proj(garment_features)\n",
    "           x = self.adaLN(x, color_hint)\n",
    "           x = self.texture_attn(x, garment_features)\n",
    "           return x\n",
    "   ```\n",
    "\n",
    "## 3. Multi-View Integration\n",
    "\n",
    "### View Correlation Module:\n",
    "- Takes 3 input views (front, side, back)\n",
    "- Processes through shared-weight encoders\n",
    "- Uses 3D-aware attention:\n",
    "  ```math\n",
    "  Attention(Q,K,V) = softmax(\\frac{QK^T}{\\sqrt{d}} + M_{3D})V\n",
    "  ```\n",
    "  Where `M_{3D}` is a learnable relative viewpoint matrix\n",
    "\n",
    "### Training Process:\n",
    "1. **Per-View Denoising**:\n",
    "   - Individual denoising for each view\n",
    "   - Shares most weights except view-specific norms\n",
    "\n",
    "2. **Consistency Loss**:\n",
    "   ```math\n",
    "   L_{consist} = \\sum_{i≠j}||Φ(v_i) - Φ(v_j)||_1\n",
    "   ```\n",
    "   Where Φ denotes deep features from a pretrained VGG network\n",
    "\n",
    "## Implementation Details\n",
    "\n",
    "### Critical Hyperparameters:\n",
    "- Diffusion steps: 1000\n",
    "- Noise schedule: Cosine\n",
    "- Latent dimension: 256×256×4\n",
    "- Batch size: 8 (per view)\n",
    "- Learning rate: 1e-5 (AdamW)\n",
    "\n",
    "### Memory Optimization:\n",
    "- Gradient checkpointing\n",
    "- Mixed precision training\n",
    "- Per-view sequential processing during inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1fefb7",
   "metadata": {},
   "source": [
    "# **Simplified Explanation of MV-VTON Architecture**  \n",
    "\n",
    "Let’s break down MV-VTON into easy-to-understand parts. Think of it like a **virtual dressing room** where you take a piece of clothing and digitally \"wear\" it on a person in different angles (front, side, back).  \n",
    "\n",
    "---\n",
    "\n",
    "## **1. The Two Main Steps**  \n",
    "MV-VTON works in **two stages**:  \n",
    "\n",
    "1. **Cloth Warping Stage** – Stretches/moves the clothing to fit the person’s pose.  \n",
    "2. **Try-On Synthesis Stage** – Uses a **diffusion model** (like AI image generators) to make the clothing look realistic on the person.  \n",
    "\n",
    "---\n",
    "\n",
    "## **2. Cloth Warping Stage (Step 1: Adjusting the Cloth)**  \n",
    "\n",
    "### **What it does:**  \n",
    "- Takes a **flat image of clothing** (e.g., a T-shirt on a white background).  \n",
    "- Predicts how it should **bend, fold, and stretch** to fit the person’s body.  \n",
    "\n",
    "### **How it works:**  \n",
    "- Uses a **neural network** to predict a **\"flow field\"** (like a digital map that says: *\"Move this part of the shirt to the right, stretch this part down\"*).  \n",
    "- The warping is **pose-aware**, meaning it looks at the person’s body position (arms up, legs crossed, etc.).  \n",
    "- Ensures the cloth **doesn’t look distorted** when applied.  \n",
    "\n",
    "### **Example:**  \n",
    "- If the person raises their arm, the sleeve of the shirt should **naturally bend** instead of staying stiff.  \n",
    "\n",
    "---\n",
    "\n",
    "## **3. Try-On Synthesis Stage (Step 2: Making It Look Real)**  \n",
    "\n",
    "This is where the **diffusion model** comes in.  \n",
    "\n",
    "### **What is a Diffusion Model?**  \n",
    "- A type of AI that **starts with noise** (random pixels) and **slowly refines it** into a realistic image.  \n",
    "- (Think of it like an artist who first sketches roughly, then adds details.)  \n",
    "\n",
    "### **How MV-VTON Uses It:**  \n",
    "1. **Input:** The **warped cloth + person’s photo**.  \n",
    "2. **Process:** The diffusion model **\"paints\"** the clothing onto the person realistically.  \n",
    "   - It fixes seams, shadows, and wrinkles.  \n",
    "   - Makes sure the cloth **matches lighting and body shape**.  \n",
    "3. **Multi-View Support:**  \n",
    "   - Unlike older methods (which only work for front view), MV-VTON can **generate side and back views** too.  \n",
    "   - Uses **\"multi-view attention\"** to keep the clothing consistent across angles.  \n",
    "\n",
    "### **Example:**  \n",
    "- If you’re wearing a **striped shirt**, the stripes should **curve naturally** around your body in all views.  \n",
    "\n",
    "---\n",
    "\n",
    "## **4. Why Diffusion Models Work Better Than GANs (Old Method)**  \n",
    "\n",
    "| Feature | GAN-Based Try-On | MV-VTON (Diffusion) |\n",
    "|---------|----------------|----------------|\n",
    "| **Realism** | Can look fake or blurry | More detailed & natural |\n",
    "| **Multi-View** | Only works for one angle | Front, side, back views |\n",
    "| **Cloth Details** | May lose patterns/textures | Keeps designs sharp |\n",
    "| **Pose Handling** | Struggles with complex poses | Works better for arms up, sitting, etc. |\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Simple Summary of How It Works**  \n",
    "\n",
    "1. **Input:**  \n",
    "   - A **photo of a person** + **photo of clothing**.  \n",
    "2. **Step 1 (Warping):**  \n",
    "   - The AI **adjusts the clothing** to fit the person’s pose.  \n",
    "3. **Step 2 (Diffusion):**  \n",
    "   - The AI **refines the image** to look like the person is really wearing it.  \n",
    "4. **Output:**  \n",
    "   - A **realistic image** of the person wearing the clothes, from **multiple angles**.  \n",
    "\n",
    "---\n",
    "\n",
    "## **6. Challenges & Limitations**  \n",
    "\n",
    "✅ **Pros:**  \n",
    "- More realistic than older methods.  \n",
    "- Works for **different body poses & views**.  \n",
    "\n",
    "❌ **Cons:**  \n",
    "- Slower than GANs (takes more time to generate).  \n",
    "- Needs **good-quality input images**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Final Thought:**  \n",
    "MV-VTON is like a **smart digital tailor**—it takes clothes, adjusts them to fit your body, and makes them look natural in photos from any angle.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b7eba8",
   "metadata": {},
   "source": [
    "## 1. Explain how to **implement a basic version** of this?  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0912f24d",
   "metadata": {},
   "source": [
    "# Implementing a Basic Virtual Try-On System\n",
    "\n",
    "Let's build a simplified version of MV-VTON that works for frontal poses. This implementation will use Python with PyTorch and focus on core functionality.\n",
    "\n",
    "## 1. Prerequisites\n",
    "\n",
    "First, install required packages:\n",
    "```bash\n",
    "pip install torch torchvision opencv-python Pillow numpy\n",
    "```\n",
    "\n",
    "## 2. Simplified Architecture\n",
    "\n",
    "We'll implement these key components:\n",
    "1. Pose estimation (to understand body shape)\n",
    "2. Clothing segmentation (to isolate garments)\n",
    "3. Basic warping (to fit clothes to body)\n",
    "4. Diffusion-based refinement (to make it look realistic)\n",
    "\n",
    "## 3. Step-by-Step Implementation\n",
    "\n",
    "### A. Pose Estimation (Using MediaPipe)\n",
    "\n",
    "```python\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "def detect_pose(image):\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(static_image_mode=True)\n",
    "    \n",
    "    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    keypoints = []\n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            keypoints.append((landmark.x, landmark.y))\n",
    "    \n",
    "    return keypoints\n",
    "```\n",
    "\n",
    "### B. Clothing Segmentation (Simplified U-Net)\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "        self.final = nn.Conv2d(64, 1, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.enc1(x)\n",
    "        x = self.dec1(x)\n",
    "        return torch.sigmoid(self.final(x))\n",
    "```\n",
    "\n",
    "### C. Basic Clothing Warping\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def warp_cloth(person_img, cloth_img, person_keypoints):\n",
    "    # Simple affine transform based on shoulder and hip points\n",
    "    src_pts = np.array([[0.3, 0.1], [0.7, 0.1], [0.5, 0.9]])  # Default cloth points\n",
    "    dst_pts = np.array([\n",
    "        person_keypoints[11][:2],  # Left shoulder\n",
    "        person_keypoints[12][:2],  # Right shoulder\n",
    "        person_keypoints[23][:2]   # Mid hip\n",
    "    ])\n",
    "    \n",
    "    M = cv2.getAffineTransform(\n",
    "        np.float32(src_pts * [cloth_img.shape[1], cloth_img.shape[0]]),\n",
    "        np.float32(dst_pts * [person_img.shape[1], person_img.shape[0]]])\n",
    "    )\n",
    "    \n",
    "    warped = cv2.warpAffine(\n",
    "        cloth_img, M, \n",
    "        (person_img.shape[1], person_img.shape[0]),\n",
    "        flags=cv2.INTER_LINEAR\n",
    "    )\n",
    "    \n",
    "    return warped\n",
    "```\n",
    "\n",
    "### D. Simplified Diffusion Refinement\n",
    "\n",
    "```python\n",
    "class SimpleDiffusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(6, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 3, 3, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, noisy, t, cond):\n",
    "        # noisy: current noisy image\n",
    "        # t: timestep\n",
    "        # cond: conditioning (warped cloth + person)\n",
    "        x = torch.cat([noisy, cond], dim=1)\n",
    "        return self.model(x)\n",
    "```\n",
    "\n",
    "### E. Putting It All Together\n",
    "\n",
    "```python\n",
    "def virtual_try_on(person_img, cloth_img):\n",
    "    # 1. Get pose\n",
    "    keypoints = detect_pose(person_img)\n",
    "    \n",
    "    # 2. Warp clothing\n",
    "    warped_cloth = warp_cloth(person_img, cloth_img, keypoints)\n",
    "    \n",
    "    # 3. Create mask (simple threshold)\n",
    "    gray = cv2.cvtColor(warped_cloth, cv2.COLOR_BGR2GRAY)\n",
    "    mask = (gray > 10).astype(np.uint8) * 255\n",
    "    \n",
    "    # 4. Diffusion refinement (simplified)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = SimpleDiffusion().to(device)\n",
    "    \n",
    "    # Convert images to tensors\n",
    "    person_tensor = torch.from_numpy(person_img).float().to(device)\n",
    "    cloth_tensor = torch.from_numpy(warped_cloth).float().to(device)\n",
    "    \n",
    "    # 5. Apply diffusion steps\n",
    "    result = cloth_tensor.clone()\n",
    "    for t in range(10):  # Normally 100+ steps\n",
    "        noise = torch.randn_like(result) * (1 - t/10)\n",
    "        result = model(result + noise, t, torch.cat([person_tensor, cloth_tensor]))\n",
    "    \n",
    "    # 6. Blend with original image\n",
    "    final = person_img * (1 - mask/255) + result.cpu().numpy() * (mask/255)\n",
    "    \n",
    "    return final\n",
    "```\n",
    "\n",
    "## 4. How to Use It\n",
    "\n",
    "```python\n",
    "# Load images\n",
    "person = cv2.imread('person.jpg')\n",
    "cloth = cv2.imread('tshirt.jpg')\n",
    "\n",
    "# Run virtual try-on\n",
    "result = virtual_try_on(person, cloth)\n",
    "\n",
    "# Save result\n",
    "cv2.imwrite('result.jpg', result)\n",
    "```\n",
    "\n",
    "## 5. Limitations of This Basic Version\n",
    "\n",
    "1. Only works for frontal poses\n",
    "2. Simple warping may distort patterns\n",
    "3. Diffusion refinement is minimal\n",
    "4. No multi-view support\n",
    "\n",
    "## Next Steps to Improve\n",
    "\n",
    "1. Replace simple warping with Thin Plate Spline (TPS) transformation\n",
    "2. Use a pre-trained diffusion model (like Stable Diffusion)\n",
    "3. Add proper clothing segmentation\n",
    "4. Implement pose-aware refinement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "218522ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "def detect_pose(image):\n",
    "    mp_pose = mp.solutions.pose\n",
    "    pose = mp_pose.Pose(static_image_mode=True)\n",
    "    \n",
    "    results = pose.process(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    keypoints = []\n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "        for landmark in results.pose_landmarks.landmark:\n",
    "            keypoints.append((landmark.x, landmark.y))\n",
    "    \n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1de7dd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def warp_cloth(person_img, cloth_img, person_keypoints):\n",
    "    # Simple affine transform based on shoulder and hip points\n",
    "    src_pts = np.array([[0.3, 0.1], [0.7, 0.1], [0.5, 0.9]])  # Default cloth points\n",
    "    dst_pts = np.array([\n",
    "        person_keypoints[11][:2],  # Left shoulder\n",
    "        person_keypoints[12][:2],  # Right shoulder\n",
    "        person_keypoints[23][:2]   # Mid hip\n",
    "    ])\n",
    "    \n",
    "    M = cv2.getAffineTransform(\n",
    "        np.float32(src_pts * [cloth_img.shape[1], cloth_img.shape[0]]),\n",
    "        np.float32(dst_pts * [person_img.shape[1], person_img.shape[0]])\n",
    "    )\n",
    "    \n",
    "    warped = cv2.warpAffine(\n",
    "        cloth_img, M, \n",
    "        (person_img.shape[1], person_img.shape[0]),\n",
    "        flags=cv2.INTER_LINEAR\n",
    "    )\n",
    "    \n",
    "    return warped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e544041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Upsample(scale_factor=2)\n",
    "        )\n",
    "        self.final = nn.Conv2d(64, 1, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.enc1(x)\n",
    "        x = self.dec1(x)\n",
    "        return torch.sigmoid(self.final(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b53abe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDiffusion(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(6, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 3, 3, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, noisy, t, cond):\n",
    "        # noisy: current noisy image\n",
    "        # t: timestep\n",
    "        # cond: conditioning (warped cloth + person)\n",
    "        x = torch.cat([noisy, cond], dim=1)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe293154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def virtual_try_on(person_img, cloth_img):\n",
    "    # 1. Get pose\n",
    "    keypoints = detect_pose(person_img)\n",
    "    \n",
    "    # 2. Warp clothing\n",
    "    warped_cloth = warp_cloth(person_img, cloth_img, keypoints)\n",
    "    \n",
    "    # 3. Create mask (simple threshold)\n",
    "    gray = cv2.cvtColor(warped_cloth, cv2.COLOR_BGR2GRAY)\n",
    "    mask = (gray > 10).astype(np.uint8) * 255\n",
    "    \n",
    "    # 4. Diffusion refinement (simplified)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = SimpleDiffusion().to(device)\n",
    "    \n",
    "    # Convert images to tensors\n",
    "    person_tensor = torch.from_numpy(person_img).float().to(device)\n",
    "    cloth_tensor = torch.from_numpy(warped_cloth).float().to(device)\n",
    "    \n",
    "    # 5. Apply diffusion steps\n",
    "    result = cloth_tensor.clone()\n",
    "    for t in range(10):  # Normally 100+ steps\n",
    "        noise = torch.randn_like(result) * (1 - t/10)\n",
    "        result = model(result + noise, t, torch.cat([person_tensor, cloth_tensor]))\n",
    "    \n",
    "    # 6. Blend with original image\n",
    "    final = person_img * (1 - mask/255) + result.cpu().numpy() * (mask/255)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8bb163b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1747162128.853981 1407373 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1747162128.873274 1409326 gl_context.cc:369] GL version: 3.2 (OpenGL ES 3.2 NVIDIA 535.230.02), renderer: NVIDIA GeForce RTX 4060 Laptop GPU/PCIe/SSE2\n",
      "W0000 00:00:1747162128.917554 1409305 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1747162128.945175 1409323 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1747162128.963603 1409316 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Expected size 490 but got size 980 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Proceed only if both images loaded\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m person \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m cloth \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mvirtual_try_on\u001b[49m\u001b[43m(\u001b[49m\u001b[43mperson\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcloth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     cv2\u001b[38;5;241m.\u001b[39mimwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresult.jpg\u001b[39m\u001b[38;5;124m'\u001b[39m, result)\n",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m, in \u001b[0;36mvirtual_try_on\u001b[0;34m(person_img, cloth_img)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):  \u001b[38;5;66;03m# Normally 100+ steps\u001b[39;00m\n\u001b[1;32m     23\u001b[0m     noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn_like(result) \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m t\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mperson_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcloth_tensor\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# 6. Blend with original image\u001b[39;00m\n\u001b[1;32m     27\u001b[0m final \u001b[38;5;241m=\u001b[39m person_img \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m mask\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m) \u001b[38;5;241m+\u001b[39m result\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m*\u001b[39m (mask\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/vton/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/vton/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[4], line 16\u001b[0m, in \u001b[0;36mSimpleDiffusion.forward\u001b[0;34m(self, noisy, t, cond)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, noisy, t, cond):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;66;03m# noisy: current noisy image\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# t: timestep\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# cond: conditioning (warped cloth + person)\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnoisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Expected size 490 but got size 980 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# Load images\n",
    "person = cv2.imread('/home/ahmad10raza/Downloads/Data Science/Projects/realtime-virtual-try-on/person.jpg')\n",
    "cloth = cv2.imread('/home/ahmad10raza/Downloads/Data Science/Projects/realtime-virtual-try-on/tsirt.jpg')\n",
    "\n",
    "# Check if images loaded\n",
    "if person is None:\n",
    "    print(\"Error: Could not load person.jpg\")\n",
    "if cloth is None:\n",
    "    print(\"Error: Could not load tsirt.jpg\")\n",
    "\n",
    "# Proceed only if both images loaded\n",
    "if person is not None and cloth is not None:\n",
    "    result = virtual_try_on(person, cloth)\n",
    "    cv2.imwrite('result.jpg', result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3db5bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def virtual_try_on(person, cloth):\n",
    "    # Resize cloth to match person's dimensions\n",
    "    cloth_resized = cv2.resize(cloth, (person.shape[1], person.shape[0]))\n",
    "\n",
    "    # Dummy overlay example (replace this with actual overlay logic)\n",
    "    blended = cv2.addWeighted(person, 0.7, cloth_resized, 0.3, 0)\n",
    "    return blended\n",
    "\n",
    "# Load images\n",
    "person = cv2.imread('/home/ahmad10raza/Downloads/Data Science/Projects/realtime-virtual-try-on/person.jpg')\n",
    "cloth = cv2.imread('/home/ahmad10raza/Downloads/Data Science/Projects/realtime-virtual-try-on/tsirt.jpg')\n",
    "\n",
    "# Check if images loaded\n",
    "if person is None:\n",
    "    print(\"Error: Could not load person.jpg\")   \n",
    "if cloth is None:\n",
    "    print(\"Error: Could not load tsirt.jpg\")\n",
    "\n",
    "# Proceed only if both images loaded\n",
    "if person is not None and cloth is not None:\n",
    "    result = virtual_try_on(person, cloth)\n",
    "    cv2.imwrite('result.jpg', result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271b59e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vton",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
